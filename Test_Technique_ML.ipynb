{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Quel(le) data scientist êtes-vous ?\n",
    "## Contexte de l’analyse\n",
    "\n",
    "Elu métier le plus sexy par la Harvard Business Review en octobre 2012, le data scientist représente un profil rare qui exige de nombreuses compétences.\n",
    "\n",
    "A partir d'un dataset Aquila, vous réaliserez :\n",
    "- un clustering non supervisé afin d'identifier 2 groupes de profils techniques distinctes\n",
    "- une prédiction des profils dont le métier n'est pas labellisé\n",
    "\n",
    "\n",
    "## Données\n",
    "data.csv contient 6 variables : \n",
    "    - 'Entreprise' correspond à une liste d'entreprises fictive\n",
    "    - 'Metier' correspond au métier parmi data scientist, lead data scientist, data engineer et data architecte\n",
    "    - 'Technologies' correspond aux compétences maîtrisées par le profil\n",
    "    - 'Diplome' correspond à son niveau scolaire (Bac, Master, PhD,...)\n",
    "    - 'Experience' correspond au nombre d'années d'expériences\n",
    "    - 'Ville' correspond au lieu de travail\n",
    "    \n",
    "\n",
    "\n",
    "## Répondez aux questions \n",
    "\n",
    "Bonne chance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import des libraries classique (numpy, pandas, ...)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, silhouette_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Importer le tableau de données dans un dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est interessant ici d'ouvrir au préalable le fichier csv avec un logiciel comme<br>\n",
    "<b>notepad++</b> afin de voir les specificités du document (separateur, etc....)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import du dataframe \"data.csv\"\n",
    "filename = 'data.csv'\n",
    "df = pd.read_csv(filename, sep=',', decimal=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Combien y a t-il d'observations dans ce dataset? Y a t-il des valeurs manquantes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methode: avec shape\n",
    "message = 'methode: nombre d\\'observations du dataset {} \\n'\n",
    "print(message.format(df.shape[0]))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe donc dans le dataset des valeurs manquantes. Cherchons à en determiner les proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculons la proportion de valeurs manquantes dans chacune des columnes\n",
    "print('proportion des valeurs manquantes dans chacune des columnes en %: \\n')\n",
    "print(df.isnull().mean()*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Concernant les valeurs manquantes, je constate que les variables categorielles <b> Entreprise</b>, <b>Metier</b> en contiennent approximativement et respectivement <b>0.17%</b> et <b>0.18%</b> tandis que la variable numerique <b>Experience</b> en contient moins de <b>1%</b>.\n",
    "<br> Ces donnees manquantes sont assez rares (moins de 1%) en general, le dataset est donc exploitable et on peut remplacer ces valeurs manquantes par diverses méthodes (moyenne, medianne, mode (pour les variables categorielles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Réaliser l'imputation des valeurs manquantes pour la variable \"Experience\" avec : \n",
    "- la valeur médiane pour les data scientists\n",
    "- la valeur moyenne pour les data engineers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_metier=df[df['Metier']=='Data scientist']\n",
    "#median=df_metier.apply(pd.DataFrame.mean,axis=0)\n",
    "median=df_metier.median(axis=0)\n",
    "experience_median=median[\"Experience\"]\n",
    "df_metier.loc[df_metier[\"Experience\"].isnull().loc[:\"Experience\"],\"Experience\"]=experience_median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def making_imputation(dataframe, value, method):\n",
    "    \"\"\" remplacer les valeurs manquantes\n",
    "    Parametres\n",
    "    ----------\n",
    "    dataframe : dataframe avec valeurs manquantes\n",
    "    value: metiers dans Metier (Data scientist,etc ...)\n",
    "    method : methode de remplacement (mean, mediane etc..)\n",
    "    Retour\n",
    "    -------\n",
    "    df : return new dataframe with replaced values\n",
    "    \"\"\"\n",
    "\n",
    "    # faire une copie du dataset\n",
    "    df = dataframe.copy()\n",
    "    # faire des verifications quant aux entrees\n",
    "    message1 = 'valeur non presente dans Experience'\n",
    "    message2 = 'methode inconnue'\n",
    "    assert value in np.unique(df.Metier.dropna()), message1\n",
    "    assert method in ['mean', 'median'], message2\n",
    "    # rechercher les indices qui satisfassent la condition\n",
    "    indices = np.where((df.Metier == value) & (df.Experience.isnull()))[0]\n",
    "    # la methode de remplacement\n",
    "    if method == 'mean':\n",
    "        moyenne = round(\n",
    "            np.mean(df.Experience.iloc[np.where(df.Metier == value)]), 2\n",
    "        )\n",
    "        df.loc[indices, 'Experience'] = moyenne\n",
    "    elif method == 'median':\n",
    "        moyenne = round(\n",
    "            np.mean(df.Experience.iloc[np.where(df.Metier == value)]), 2\n",
    "        )\n",
    "        df.loc[indices, 'Experience'] = moyenne\n",
    "    return df\n",
    "\n",
    "\n",
    "# Remplacer les valeurs manquantes pour les data scientists par la mediane\n",
    "df = making_imputation(df, 'Data scientist', 'median')\n",
    "# Remplacer les valeurs manquantes pour les data engineers par la moyenne\n",
    "df = making_imputation(df, 'Data engineer', 'mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode que j'ai employee ici, consiste d'abord a detecter <b> les indices </b> du dataset correspondant aux differents emplois qu'on recherche ici, à savoir <b> data scientist </b> et <b> data engineers </b>.\n",
    "<br> Ensuite nous allons proceder à la modification des <b> NaN </b> presents à ces ligne par les valeurs demandées.\n",
    "<br> J'utiliserai ici compte tenu de la taille du dataset (moins de 10.000 lignes) les fonctions de <b>numpy</b> qui donnent de meilleurs performances comparativement à celles de <b>pandas</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Combien d'années d'expériences ont, en moyenne, chacun des profils : le data scientist, le lead data scientist et le data engineer en moyenne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des annees d'experiences de chacun des profils\n",
    "print(\n",
    "    'Moyenne des experiences des differents metiers \\n {}'.format(\n",
    "        round(df.groupby('Metier')['Experience'].mean(), 2)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Le data scientist </b> a en moyenne <b>3.0 annees </b> d'experiences. <br>\n",
    "<b> Le lead data scientist </b> a en moyenne <b>3.97 annees </b> d'experiences. <br>\n",
    "<b> Le data engineer </b> a en moyenne <b>2.98 annees </b> d'experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Faire la représentation graphique de votre choix afin de comparer le nombre moyen d'années d'expériences pour chaque métier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('Metier')['Experience'].mean().plot(kind='bar')\n",
    "plt.title('Annee d\\'experiences de divers acteurs de la data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Transformer la variable continue 'Experience' en une nouvelle variable catégorielle 'Exp_label' à 4 modalités: débutant, confirmé, avancé et expert\n",
    "- Veuillez expliquer votre choix du règle de transformation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifions s'il reste encore des <b>NaN</b> dans la variable <b>Experience</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifions s'il reste encore des NaN dans experience\n",
    "df.isnull().mean()*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il reste encore <b>0.43%</b> de NaN dans experience, certainement du aux valeurs dans Lead data scientist et lead architect qu'on a pas modifié.\n",
    "<br> Afin de transformer la variable categorielle, nous allons aussi proceder à l'imputation de ces <b>NaN</b> avec\n",
    "la mediane, plus robuste aux valeurs abberantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remplacer les valeurs manquantes pour les data architectes par la mediane\n",
    "df = making_imputation(df, 'Lead data scientist', 'median')\n",
    "# Remplacer les valeurs manquantes pour les lead data scientist par la moyenne\n",
    "df = making_imputation(df, 'Data architecte', 'median')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Explication de la methode de transformation </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La categorisation d'une variabe continue s'accompagne toujours d'une perte d'informations mais elle peut accelerer et ameliorer les performances du modèle predictif.\n",
    "<br> Nous allons utiliser ici une méthode basée sur les quantiles mais inégalement repartis.\n",
    "    <br> Arbitrairement nous allons ici considerer qu'environ <b>40%</b> de la population appartient à la classe <b> debutant </b>, <b>30%</b> à la classe <b>confirme</b>, <b>15%</b> à la classe <b>avance</b> et les  <b>5% restants</b> à la classe <b>expert</b>. Cela me semble assez réaliste\n",
    "<br> La méthode cut quant à elle créer des clusters successifs mais ne contenant pas forcement le meme nombre d'individus. Cette méthode nous parait plus réaliste.\n",
    "<br> Une dernière méthode consisterait ici à utiliser des connaissances à priori pour effectuer cette classfication.\n",
    "    <br> Nous choisissons ici d'uliser la méthode <b> qcut </b> de pandas plus adapté à notre problematique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realisons ala categorisation avec la methode cut\n",
    "df['Exp_label'] = df.groupby('Metier')['Experience'].apply(\n",
    "    lambda x: pd.qcut(\n",
    "        x, q=[0, .4, .7, .95, 1], labels=[\"debutant\", \"confirme\", \"avance\", \"expert\"]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plutot que de categoriser ici de manière generale, nous allons créer les categories en fonctions du métier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(df.Exp_label.value_counts() / df.shape[0]).plot(kind=\"bar\")\n",
    "plt.title('repartition des categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Quelles sont les 5 technologies les plus utilisées? Faites un graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listons l'ensemble des technologies utilisées par l'ensemble des candidats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons si sur chaque ligne, chaque observation ne contient pas une technoogie en double ce qui peut fausser le calcul du decompte.\n",
    "<br> On se sert de set pour faire le compte des technoogies uniques et nous verifions si ce compte est egale à celle de la liste de depart si oui il n'ya pas de double sinon il y'en a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisons la liste unique de l'ensemble des technologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs choses sont a noter après visualisation de la colonne <b> Technologie</b> : \n",
    "<br> L'anglais n'est pas une technologie donc n'a theoriquement pas sa place parmi les technologies\n",
    "<br> Des technologies comme <b> Scikit learn </b> sont mentionnés en double a cause des majuscules\n",
    "<br> Machine learning, big data, map reduce sont des domaines et un paradigme pour le dernier et non pas des technologies au sens propre\n",
    "<br> <b> Solutions envisagees </b>\n",
    "<br> <b>1.</b> Mettre en miniscule dans le dataset pour eviter les doublons\n",
    "<br> <b>2.</b> Pour le listings des 5 technologies les plus utilisees enlever l'Anglais du dataset ainsi que les termes se referant vaguement au domaine (Machine learning, big data, deep learning, scoring).\n",
    "<br> <b>3.</b> Aussi les termes Hadoop(HDFS) et Hadoop se referent à l'utilisation de la technologie Hadoop. Il y'a donc possibilite de les regrouper. De meme HDFS est le systeme de fichier de Hadoop donc se refère à cette technologie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mettre en miniscule les technologies \n",
    "df[\"Technologies_reelles\"] = df.Technologies.transform(lambda x: x.lower())\n",
    "# Remplacons les termes Hadoop(HDFS), HDFS et Hadoop par un unique terme hadoop\n",
    "df.Technologies_reelles = df.Technologies_reelles.transform(\n",
    "    lambda x: re.sub(\"(.+\\(hdfs\\)|hdfs)\", \"hadoop\", x)\n",
    ")\n",
    "# Effacons les termes anglais, machine learning, etc..\n",
    "df.Technologies_reelles = df.Technologies_reelles.transform(\n",
    "    lambda x: re.sub(\n",
    "        \"(anglais|big data|machine learning|deep learning|scoring|ai\\\n",
    "         |map-reduce)\",\n",
    "        \"\",\n",
    "        x,\n",
    "    )\n",
    ")\n",
    "# creeons un dataset avec chacune des technologies comme columne\n",
    "mlb = MultiLabelBinarizer()\n",
    "technologies = pd.DataFrame(\n",
    "    mlb.fit_transform(df.Technologies_reelles.str.split(\"/\")),\n",
    "    columns=mlb.classes_,\n",
    "    index=df.index,\n",
    ")\n",
    "# enlever la columne nulle\n",
    "technologies = technologies.drop(\"\", axis=1)\n",
    "# verifions si une technologie n'est pas presente 2 fois sur une meme ligne\n",
    "assert len(np.unique(technologies)) == 2, \"presence de doublons\"\n",
    "# mettre en minuscule le texte\n",
    "technologie_count = technologies.agg(\"sum\").sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'les 5 technos les plus utilisees sont {}'\n",
    "print(message.format(technologie_count.index.values[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette liste me parait convenable au vu de la definition de technologies et logiciels meme si certains termes peuvent être regrouper ensemble comme python et ses librairies ou la question qu'il aurait à poser est \n",
    "qu'es ce la maitrise de python?\n",
    "<br> Nous allons evoluer avec cette liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_most_used_technologies(number, technologie, other_include=True):\n",
    "    \"\"\" tracer un camembert\n",
    "    Parametres\n",
    "    ----------\n",
    "    number : number of technologies\n",
    "    technologies : liste des technologies\n",
    "    other_include : inclure oui ou non les autres technos\n",
    "                    dans le graphique\n",
    "    Retour\n",
    "    -------\n",
    "    retourne un camembert\n",
    "    \"\"\"\n",
    "    n_used_techno_names = np.asarray(technologie.index[:number+1])\n",
    "    n_used_techno_names = np.append(\n",
    "        n_used_techno_names, \"Autres technos\"\n",
    "    )  # rajouter les autres qu'on a pas comptabiliser\n",
    "    n_used_techno_count = np.asarray(technologie[: number + 1])\n",
    "    n_used_techno_count = np.append(\n",
    "        n_used_techno_count, np.sum(technologie.values[number+1:])\n",
    "    )\n",
    "    # tracer du camembert\n",
    "    if other_include:\n",
    "        fig1, ax1 = plt.subplots()\n",
    "        ax1.pie(\n",
    "            n_used_techno_count,\n",
    "            labels=n_used_techno_names,\n",
    "            autopct=\"%1.1f%%\",\n",
    "            shadow=True,\n",
    "            startangle=180,\n",
    "        )\n",
    "        ax1.axis(\"equal\")\n",
    "        ax1.set_title(str(number) + \" technos les plus utilisees + les autres\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig2, ax2 = plt.subplots()\n",
    "        ax2.pie(\n",
    "            n_used_techno_count[:-2],\n",
    "            labels=n_used_techno_names[:-2],\n",
    "            autopct=\"%1.1f%%\",\n",
    "            shadow=True,\n",
    "            startangle=180,\n",
    "        )\n",
    "        ax2.axis(\"equal\")\n",
    "        ax2.set_title(\n",
    "            \"uniquement les \" + str(number) + \" technos les plus utilisees\"\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_horizontal_histogram(number, technologie, other_include=True):\n",
    "    \"\"\" tracer un histograme horizontal\n",
    "    Parametres\n",
    "    ----------\n",
    "    number : number of technologies\n",
    "    technologies : liste des technologies\n",
    "    other_include : inclure oui ou non les autres technos\n",
    "                    dans le graphique\n",
    "    Retour\n",
    "    -------\n",
    "    retourne un histogramme horizontal\n",
    "    \"\"\"\n",
    "    n_used_techno_names = np.asarray(technologie.index[: number + 1])\n",
    "    n_used_techno_names = np.append(\n",
    "        n_used_techno_names, \"Autres technos\"\n",
    "    )  # rajouter les autres qu'on a pas comptabiliser\n",
    "    n_used_techno_count = np.asarray(technologie[: number + 1])\n",
    "    n_used_techno_count = np.append(\n",
    "        n_used_techno_count, np.sum(technologie.values[number+1:])\n",
    "    )\n",
    "    if other_include:\n",
    "        plt.figure(num=0)\n",
    "        plt.title(\n",
    "            \"Uniquement les  \" +\n",
    "            str(number) +\n",
    "            \" technos les plus utilisees + les autres\"\n",
    "        )\n",
    "        ax1 = sb.barplot(\n",
    "            x=n_used_techno_count / np.sum(n_used_techno_count),\n",
    "            y=n_used_techno_names,\n",
    "        )\n",
    "    else:\n",
    "        plt.figure(num=0)\n",
    "        plt.title(\n",
    "            \"Uniquement les  \" + str(number) + \" technos les plus utilisees\"\n",
    "        )\n",
    "        ax2 = sb.barplot(\n",
    "            x=n_used_techno_count[:-2] / np.sum(n_used_techno_count[:-2]),\n",
    "            y=n_used_techno_names[:-2],\n",
    "        )\n",
    "\n",
    "# 5 premieres technos  à afficher\n",
    "NOMBRE_TECHNOS = 5\n",
    "# inclure oui ou non  les autres technos dans le graphique\n",
    "INCLUDE_OTHER_TECHNOS = False\n",
    "# tracé des differents graphiques\n",
    "plotting_most_used_technologies(\n",
    "    NOMBRE_TECHNOS, technologie_count, INCLUDE_OTHER_TECHNOS\n",
    ")\n",
    "plot_horizontal_histogram(\n",
    "    NOMBRE_TECHNOS, technologie_count, INCLUDE_OTHER_TECHNOS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion <br>Comme on peut le constater sur le camembert et le barchart <br> Python vient en première position des languages utilisés  par les personnes de la data avec un score de <b>15.3%</b>, suivi de près par le langage R, prisée des statisticiens <b>10.0%</b>, Ensuite viennent des outils de gestion et de requetage de la donnee comme SQL <b>5.9%</b>, Hadoop <b>3.6%</b>. <br> Java lui est moins utilisé comparé à Python et R, cela peut être lié au fait que les outils orientés Machine Learning sont plus rares ou moins vulgarisés (prisés)\n",
    "<br> Ces scores representent les taux par rapport à l'ensemble des logiciels mentionnes. Nous nous sommes assurés que chaque observation ne contient aucun doublon. <br> Les <b>N+1</b> dernières technologies ont été rangés dans <b> Autres technos </b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Réaliser une méthode de clustering non supervisée de votre choix pour faire apparaître 2 clusters que vous jugerez pertinents. Donnez les caractéristiques de chacun des clusters.\n",
    "-  Justifier la performance de votre algorithme grace à une métrique.\n",
    "-  Interpréter votre resultat.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Le clustering que nous allons realisé ici vise à faire apparaitre deux groupes de profils techniques distincts\n",
    "<br> A cet effet, l'entreprise, le metier, l'experience ne sont pas à mon avis des variables essentielles\n",
    "<br> Nous allons creer des <b>dummies variables</b> avec la variable <b>Technologies_reelles</b>. A cela nous ajouterons le diplome qui est aussi determinant pour connaitre les profils techniques.\n",
    "<br> A cet effet, nous utiliserons un autre dataset que nous nomerons <b> profil_technique</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creation de dummies variables avec les metiers\n",
    "metiers = pd.get_dummies(df.Metier)\n",
    "# creation de dummies variables avec l'experience\n",
    "experiences = pd.get_dummies(df.Exp_label)\n",
    "# concatenons l'ensemble de ces dataframes\n",
    "profil_technique = pd.concat([technologies, metiers, experiences], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agira ici de remplacer toute les variables categorielles par des variables numeriques avec le one hot encoding. \n",
    "On peut supprimer la variable metier pour cette etude, par ce que le metier correspont approximativement a une certaine categorie de competences techniques plus ou moins defini, l'entreprise aussi ne semble pas être une variable pertinente, de meme que la ville ou l'emploi est effectué\n",
    "Le nombre d annee d experience peut definir un profil technique on va get dummies la variable cree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction de la dimensionnalite concernant les technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enregistrer le % de variance explique\n",
    "percentage_variance_explained = np.array([])\n",
    "for elt in range(1, profil_technique.shape[1]):\n",
    "    tsvd = PCA(n_components=elt)\n",
    "    X = tsvd.fit_transform(profil_technique)\n",
    "    percentage_variance_explained = np.append(\n",
    "        percentage_variance_explained,\n",
    "        tsvd.explained_variance_ratio_[0:elt].sum(),\n",
    "    )\n",
    "# tracer une courbe en vue de la selection de variables\n",
    "plt.plot(\n",
    "    list(range(1, profil_technique.shape[1])), percentage_variance_explained\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recuperer la premiere valeur respectant la condition\n",
    "number_of_components = (\n",
    "    np.where(np.asarray(percentage_variance_explained) > 0.95)[0][0] + 1\n",
    ")\n",
    "# ne prendre que les \"number_of_components\" premieres composantes\n",
    "tsvd = PCA(n_components=number_of_components)\n",
    "# effectuer une PCA\n",
    "software_maitrise_to_take = tsvd.fit_transform(profil_technique)\n",
    "print(\n",
    "    \"pourcentage explique {} \\n\".format(\n",
    "        tsvd.explained_variance_ratio_[0:number_of_components].sum()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation du KMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul qualite du clustering avant reduction dimensionnalite\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=2, n_init=10)\n",
    "preds_kmean = kmeans.fit_predict(profil_technique)\n",
    "clustering_validity = silhouette_score(profil_technique, preds_kmean)\n",
    "print(\"avant reduction dimensionnalite {} \\n\".format(clustering_validity))\n",
    "\n",
    "# calcul qualite du clustering apres reduction dimensionnalite\n",
    "kmeans2 = KMeans(init=\"k-means++\", n_clusters=2, n_init=10)\n",
    "preds2_kmean = kmeans.fit_predict(software_maitrise_to_take)\n",
    "clustering_validity2 = silhouette_score(\n",
    "    software_maitrise_to_take, preds2_kmean\n",
    ")\n",
    "print(\"apres reduction dimensionnalite {} \\n\".format(clustering_validity2))\n",
    "# creation d'une columne pour le resultat de prediction\n",
    "df[\"profil_technique_kmean\"] = preds2_kmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupe_0_index = df.query(\"profil_technique_kmean==0\").index.values\n",
    "technos_0 = (\n",
    "    technologies.iloc[groupe_0_index, :]\n",
    "    .agg(\"sum\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "groupe_1_index = df.query(\"profil_technique_kmean==1\").index.values\n",
    "technos_1 = (\n",
    "    technologies.iloc[groupe_1_index, :]\n",
    "    .agg(\"sum\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "# tracer les 5 technos dominantes du groupe 0\n",
    "plotting_most_used_technologies(5, technos_0, INCLUDE_OTHER_TECHNOS)\n",
    "print(\"================================================ \\n\")\n",
    "# tracer les 5 technos dominantes du groupe 1\n",
    "plotting_most_used_technologies(5, technos_1, INCLUDE_OTHER_TECHNOS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La qualité du clustering est assez <b> faible </b> et ce malgré la reduction de dimensionnalité qui ameliore ici sensiblement le resultat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Essayons de trouver à quoi corresponde  ces clusters en terme de denomination metier\n",
    "Je pars de l'hypothèse que le clustering a separer les données en 2 en procedant ainsi,\n",
    "d'une part les data scientist qui sont charger à partir des données, de les analyser, de les pré traiter et de mettre en place des modèles et d'autres part les autres acteurs de la data à savoir les data architectes et les data engineer qui eux mettre en place des moyens afin d'avoir des données propres, accessibles et exploitables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decouvrons la composition des differents groupes en terme de metier\n",
    "df.groupby(\"profil_technique_kmean\")[\"Metier\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Notre hypothèse de départ  </b> se voit confirmer\n",
    "Dans un groupe on a les scientist data scentist et lead data scientist (en majorite) et dans \n",
    "l'autre des specialistes de la donnee( data architecte et data engineer)\n",
    "##### qualite du clustering\n",
    "La faible qualité du clustering s'explique par  le fait que les clusters crées partagent bon nombre d'elements en commun ( de soft skills) comme <b>python</b> (technologie qui apparait en premier dans les deux groupes) ce qui fait qu'il sont difficilement separable.\n",
    "Aussi, un data scientist peut très bien s'occuper du recueil de la donnée en utilisant les technologies adequates (SQL etc...) pendant que d'autres ne le font pas, ce qui rend la separation entre ces deux classes très difficile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Réaliser la prédiction des métiers manquants dans la base de données par l'algorithme de votre choix\n",
    "-  Justifier la performance de votre algorithme grace à une métrique.\n",
    "-  Interpréter votre resultat.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation de variable interessantes\n",
    "Creer une variable qui va dire si l'individu en question a fait de longues etudes ou pas <br> <b> longue etudes:</b> (BAC+5 et plus).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def higher_ed(value):\n",
    "    if value == \"Phd\":\n",
    "        return \"Tres\"\n",
    "    elif value == \"Master\":\n",
    "        return \"Classique\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "\n",
    "df[\"Hautes_etudes\"] = df.Diplome.apply(higher_ed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recuperons les index avec valeurs manquantes de la columne 'Metier'\n",
    "metiers_manquant = df[pd.isnull(df.Metier)]\n",
    "index_metiers_manquants = np.asarray(metiers_manquant.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"selectionner des columne(s) du dataframe\"\"\"\n",
    "\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "    def fit(self, attribute_names):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"creer des dummies variables\n",
    "        Parametres\n",
    "        ----------\n",
    "        X : dataframe a utiliser\n",
    "\n",
    "        Retour\n",
    "        -------\n",
    "        dummies_array: dummies array\n",
    "        \"\"\"\n",
    "        encoder = OneHotEncoder()\n",
    "        dummies_array = pd.get_dummies((X[self.attribute_names]))\n",
    "        return dummies_array\n",
    "\n",
    "\n",
    "class MultiLabelDataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" selection des columne multi label\"\"\"\n",
    "\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "    def fit(self, attribute_names):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"creer des dummies variables\n",
    "        Parametres\n",
    "        ----------\n",
    "        X : dataframe a utiliser\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dummies_array: dummies array\n",
    "        \"\"\"\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        dummies_array = pd.DataFrame(\n",
    "            mlb.fit_transform(X[self.attribute_names].str.split(\"/\")),\n",
    "            columns=mlb.classes_,\n",
    "        )\n",
    "        # enlever la colonne avec '' lie au split\n",
    "        dummies_array = dummies_array.drop(\"\", axis=1)\n",
    "        return dummies_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# definition de quelques fonctions qui me seront utiles\n",
    "def dummies_categorical_var(single_label_list, multi_label_list, dataframe):\n",
    "    \"\"\"creer des dummies variables a partir des variables categorielles\n",
    "    Parametres\n",
    "    ----------\n",
    "    single_label_list : column with unique label per row\n",
    "    multi_label_list : column with multiple label per row\n",
    "    dataframe : dataframe with all this columns\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numerique_df : variables numeriques issues des variables categorielles\n",
    "    metier_manquant : variables numeriques des metiers manquants\n",
    "    \"\"\"\n",
    "    # mettre en place les controles\n",
    "\n",
    "    # mettre en place des pipelines de donnees en s'appuyant sur le typage\n",
    "    # canard du Pipeline de sklearn\n",
    "    pipeline_mono = Pipeline(\n",
    "        [(\"one_hot_encoding\", DataFrameSelector(single_label_list))]\n",
    "    )\n",
    "    pipeline_multi = Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"multi_label_encoding\",\n",
    "                MultiLabelDataFrameSelector(multi_label_list),\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    finale_pipeline = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            (\"simple_hot_encoding\", pipeline_mono),\n",
    "            (\"multi label encoding\", pipeline_multi),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # convertir les variables en valeurs numeriques\n",
    "    numerique_df = finale_pipeline.fit_transform(df)\n",
    "\n",
    "    # Retirons les variables caracteeristiques avec le metier manquant\n",
    "    metier_manquant = numerique_df[index_metiers_manquants, :]\n",
    "\n",
    "    # Supprimons ces lignes de notre dataset\n",
    "    numerique_df = np.delete(numerique_df, index_metiers_manquants, axis=0)\n",
    "\n",
    "    return numerique_df, metier_manquant\n",
    "\n",
    "\n",
    "def diviser_dataset(dataframe, numerique_df, ratio_test, column_to_predict):\n",
    "    \"\"\"diviser le dataset en un jeu de d'apprentissage\n",
    "    et un jeu de test.\n",
    "    Parametres\n",
    "    ----------\n",
    "    dataframe : le dataset en question avec des variables categorielles\n",
    "    dataframe_2 : dataframe avec OneHotEncoding des variables categorielles\n",
    "    ratio_test : le ratio a attribuer à l'apprentissage\n",
    "    column_to_predict : columne que l'on veut pouvoir predire\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    strat_train_set : jeu d'entrainement\n",
    "    y_train_set : label d'entrainement\n",
    "    strat_test_set : jeu de test\n",
    "    y_test_set : label de test\n",
    "    \"\"\"\n",
    "\n",
    "    # mettre en place les garde fou\n",
    "\n",
    "    # supprimer de la columne à predire les valeurs NaN\n",
    "    df_base = df.dropna(subset=[column_to_predict], how=\"any\").reset_index()\n",
    "\n",
    "    # on va ici diviser le set en 80% pour l'apprentissage et 20% pour le test\n",
    "    split = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=ratio_test, random_state=42\n",
    "    )\n",
    "    for train_index, test_index in split.split(df_base, df_base.Metier):\n",
    "        strat_train_set = numerique_df[train_index, :]\n",
    "        strat_test_set = numerique_df[test_index, :]\n",
    "    strat_train_set = np.column_stack(\n",
    "        (strat_train_set, df_base.loc[train_index, \"Experience\"])\n",
    "    )\n",
    "    strat_test_set = np.column_stack(\n",
    "        (strat_test_set, df_base.loc[test_index, \"Experience\"])\n",
    "    )\n",
    "    # recuperons les variables cibles\n",
    "    y_train_set = df_base.loc[train_index, column_to_predict]\n",
    "    y_test_set = df_base.loc[test_index, column_to_predict]\n",
    "\n",
    "    return strat_train_set, y_train_set, strat_test_set, y_test_set\n",
    "\n",
    "\n",
    "def evaluate_random_forest_performance(grid_search, diff_jeu):\n",
    "    \"\"\"evaluer un modele de random forest pour la prediction\n",
    "       en selectionnant le meilleur jeu d'hyperparametres\n",
    "       possibles\n",
    "    Parametres\n",
    "    ----------\n",
    "   grid_search : ensemble des parametres grid search\n",
    "   diff jeu : tuple contenant le jeu d'apprentissage,\n",
    "              le label d'apprentissage, le jeu de test\n",
    "              et les labels de test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    chosen_model : meilleur modele possible\n",
    "    confusion_matrice : matrice de confusion sur le jeu de test\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier()\n",
    "    strat_train_set = diff_jeu[0]  # jeu d'entrainement\n",
    "    y_train_set = diff_jeu[1]  # label d'entrainement\n",
    "    strat_train_set = diff_jeu[2]\n",
    "    y_train_set = diff_jeu[3]\n",
    "    # faire une recherche des meilleurs hyperparametres avec cv=5\n",
    "    rf_random = GridSearchCV(\n",
    "        estimator=model, param_grid=grid_search, cv=5, verbose=2, n_jobs=-1\n",
    "    )\n",
    "    # entrainer l'arbre de foret aleatoires\n",
    "    rf_random.fit(strat_train_set, y_train_set)\n",
    "    # selectionner le meilleur modele à l'issu du GridSearchCV\n",
    "    chosen_model = rf_random.best_estimator_\n",
    "    # score du meilleur modele sur le jeu de validation\n",
    "    print(\n",
    "        \"score du meilleur modele sur le jeu de validation {}% \\n\".format(\n",
    "            rf_random.best_score_ * 100\n",
    "        )\n",
    "    )\n",
    "    # prediction on test_set\n",
    "    prediction_test_set = chosen_model.predict(strat_test_set)\n",
    "    print(\n",
    "        \"accuracy on test set : {}% \\n\".format(\n",
    "            accuracy_score(y_test_set, prediction_test_set) * 100\n",
    "        )\n",
    "    )\n",
    "    confusion_matrice = confusion_matrix(\n",
    "        y_test_set, prediction_test_set, labels=list(np.unique(y_test_set))\n",
    "    )\n",
    "    return chosen_model, confusion_matrice\n",
    "\n",
    "\n",
    "def prediction_metier_manquants(\n",
    "    metier_manquant, attribute_array, chosen_model\n",
    "):\n",
    "    \"\"\"predire les metiers manquants\n",
    "    Parametres\n",
    "    ----------\n",
    "    attribute_array : jeu de donnees ne contenant que les metiers manquants\n",
    "    chosen_model : modele de prediction charge d'effectuer la prediction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metiers_a_predire : dataset avec les metiers predits\n",
    "    \"\"\"\n",
    "    metiers_a_predire = metier_manquant.copy()\n",
    "    # rajout de la columne experience\n",
    "    attribute_array = np.column_stack(\n",
    "        (attribute_array, metiers_a_predire.loc[:, \"Experience\"])\n",
    "    )\n",
    "    # prediction avec le modele des classes manquantes\n",
    "    pred_classes_manquantes = chosen_model.predict(attribute_array)\n",
    "    # ajouter dans le dataset les metiers predits\n",
    "    metiers_a_predire.insert(2, \"Metiers_predits\", pred_classes_manquantes)\n",
    "    # re ordonner les index du dataset\n",
    "    metiers_a_predire = metiers_a_predire.reset_index()\n",
    "    return metiers_a_predire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choix de l'algorithme de classification\n",
    "Pour le choix de l'algorithme de classification, nous allons ici directement utiliser l'artillerie lourde et \n",
    "selectionner un algorithme de type ensembliste type <b> random forest </b>.\n",
    "<br> Ce choix se justifie notamment par le fait que l'erreur commise lors d'une tache de classification se decompose en deux.\n",
    "D'une part une <b>erreur bayesienne</b> lie a la distribution des classes et la structure conditionnelle du problème irreductible et une  deuxieme erreur dite <b>ajoutee</b> lié au classifieur qu'on choisie.\n",
    "Mettre ensemble plusieurs algorithmes de machine learning qui se corrige les uns les autres ou une decision est prise sur l'ensemble des reponses fournie (à la fin pour les <b> random forest </b> et au debut pour les algo du type <b> xgboost </b>) permet d'apporter une robustesse et une justesse supplementaire à la reponse.\n",
    "<br> <br> Le choix de <b> GridSearchCV </b> par rapport à <b> RandomizedGridSearchCV </b> se justifie ici par la taille du dataset, assez petit et par les capacités de ma machine. \n",
    "<br> Bien vrai que le <b> RandomizedGridSearch </b> s'effectue plus rapidement, il donne de moins bon resultats en general sur de petit dataset.\n",
    "<br> <br> On s'appuie sur le typage <b> canard </b> des transformateurs pipeline pour en creer des customiser\n",
    "et transformer nos variables categorielle en variable numerique\n",
    "<br> En effet un tirage purement aleatoire pourrait sous representer ou sur representer une classe dans le jeu d'apprentissage par rapport au jeu de depart ce qui poserait des problemes pour la phase de test.\n",
    "<br> <br> Dans les differents dataset d'entrainement et de test, nous allons proceder à la transformation des variables categorielles en dummies.\n",
    "<br> On l'a deja plus haut pour les variables <b> Exp_label</b>, <b>Diplome</b> <b>Metier</b> et <b>Technologies_reelles</b>\n",
    "<b> La ville et l'entreprise </b> ne definissent à priori pas le metier mais nous allons les laisser dans un premier temps et observer les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tester avec un ensemble de variable qui me semblent pertinent\n",
    "single_frame_selector = [\n",
    "    \"Diplome\",\n",
    "    \"Exp_label\",\n",
    "    \"Hautes_etudes\",\n",
    "    \"profil_technique_kmean\",\n",
    "]\n",
    "multiple_frame_selector = \"Technologies_reelles\"\n",
    "ratio_test = 0.2\n",
    "numerique_df, metier_manquant = dummies_categorical_var(\n",
    "    single_frame_selector, multiple_frame_selector, df\n",
    ")\n",
    "strat_train_set, y_train_set, strat_test_set, y_test_set = diviser_dataset(\n",
    "    df, numerique_df, ratio_test, \"Metier\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Premier modèle predictif (Arbre de decision)\n",
    "Commencons par quelque chose de plus simple de prime abord en\n",
    "utilisant un arbre de decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_param_selection(data_set, label_set, nfolds):\n",
    "    \"\"\"predire les metiers manquants\n",
    "    Parametres\n",
    "    ----------\n",
    "    data_set : jeu de donnees ne contenant que les metiers manquants\n",
    "    label_set : modele de prediction charge d'effectuer la prediction\n",
    "    nfolds : nombre de groupe pour la cross validation\n",
    "    Returns\n",
    "    -------\n",
    "    best_estimator : meilleur estimateur\n",
    "    \"\"\"\n",
    "    # strategie de split\n",
    "    splitter = [\"best\", \"random\"]\n",
    "    # nombre de feature a considerer à chaque split\n",
    "    max_features = [\"auto\", \"log2\", None]\n",
    "    # fonction pour mesurer la qualite du split\n",
    "    criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "    param_grid = {\n",
    "        \"splitter\": splitter,\n",
    "        \"max_features\": max_features,\n",
    "        \"criterion\": criterion,\n",
    "    }\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        param_grid=param_grid,\n",
    "        cv=nfolds,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(data_set, label_set)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    return best_estimator\n",
    "\n",
    "n_folds = 5\n",
    "tree_clf = tree_param_selection(strat_train_set, y_train_set, n_folds)\n",
    "# prediction on test_set\n",
    "prediction_test_set = tree_clf.predict(strat_test_set)\n",
    "print(\n",
    "    \"accuracy on test set : {}% \\n\".format(\n",
    "        accuracy_score(y_test_set, prediction_test_set) * 100\n",
    "    )\n",
    ")\n",
    "\n",
    "# calculer la matrice de confusion\n",
    "confusion_matrice = confusion_matrix(\n",
    "    y_test_set, prediction_test_set, labels=list(np.unique(y_test_set))\n",
    ")\n",
    "print('columnes matrice confusion {}'.format(list(np.unique(y_test_set))))\n",
    "print(\"matrice de confusion \\n {} \".format(confusion_matrice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La moyenne des predictions sur le jeu de test est d'environ 89%, ce qui n'est pas mal\n",
    "pour un simple arbre de decision.\n",
    "Essayons d'autres modèles comme par exemple un SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metiers_a_predire = prediction_metier_manquants(\n",
    "    metiers_manquant, metier_manquant, tree_clf\n",
    ")\n",
    "\n",
    "# avoir un apercu des predictions à effectuer\n",
    "metiers_a_predire.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test du modèle SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_param_selection(data_set, label_set, nfolds):\n",
    "    \"\"\"predire les metiers manquants\n",
    "    Parametres\n",
    "    ----------\n",
    "    data_set : jeu de donnees ne contenant que les metiers manquants\n",
    "    label_set : modele de prediction charge d'effectuer la prediction\n",
    "    nfolds : nombre de groupe pour la cross validation\n",
    "    Returns\n",
    "    -------\n",
    "    best_estimator : meilleur estimateur\n",
    "    \"\"\"\n",
    "    Cs = [0.01, 0.1, 1]\n",
    "    gammas = [0.01, 0.1, 1]\n",
    "    kernel = [\"rbf\", \"sigmoid\", \"linear\"]\n",
    "    param_grid = {\"C\": Cs, \"gamma\": gammas, \"kernel\": kernel}\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=svm.SVC(),\n",
    "        param_grid=param_grid,\n",
    "        cv=nfolds,\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(data_set, label_set)\n",
    "    best_estimator = grid_search.best_estimator_\n",
    "    return best_estimator\n",
    "\n",
    "\n",
    "modele_svm = svc_param_selection(strat_train_set, y_train_set, n_folds)\n",
    "# prediction on test_set\n",
    "prediction_test_set = modele_svm.predict(strat_test_set)\n",
    "print(\n",
    "    \"accuracy on test set : {}% \\n\".format(\n",
    "        accuracy_score(y_test_set, prediction_test_set) * 100\n",
    "    )\n",
    ")\n",
    "# calculer la matrice de confusion\n",
    "confusion_matrice = confusion_matrix(\n",
    "    y_test_set, prediction_test_set, labels=list(np.unique(y_test_set))\n",
    ")\n",
    "print('columnes matrice confusion {}'.format(list(np.unique(y_test_set))))\n",
    "print(\"matrice de confusion \\n {} \".format(confusion_matrice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'apercoit ici que l'algorithme arrive à relativement bien classer les <b> Data architecte </b> et les <b> Data engineer </b>\n",
    "<br> Il a cependant plus de mal à distinguer les <b> data scientist </b> des <b> lead data scientist </b>.\n",
    "<br> Un problème se pose pour les autres corps de metiers comme les <b>Data architecte</b> et les <b> Data engineer </b>.\n",
    "<br> En effet, il n'est pas precisé si ceux ci sont des <b>lead</b> dans leur domaines c'est à dire possèdent des qualités manageriales, ce qui peut être le cas vu le nombre d'année d'experience de certains d'eux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metiers_a_predire = prediction_metier_manquants(\n",
    "    metiers_manquant, metier_manquant, tree_clf\n",
    ")\n",
    "\n",
    "# avoir un apercu des predictions à effectuer\n",
    "metiers_a_predire.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilisation du RANDOM FOREST \n",
    "Essayons d'ameliorer les resultats précédents de l'ordre de 90% sur le jeu de test en\n",
    "privilégiant cette fois ci un algorithme dit ensembliste, le <b> random forest </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=10, stop=100, num=50)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"log2\", None]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "grid_search = {\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"max_features\": max_features,\n",
    "    \"bootstrap\": bootstrap,\n",
    "}\n",
    "\n",
    "repartition_dataset = (\n",
    "    strat_train_set,\n",
    "    y_train_set,\n",
    "    strat_test_set,\n",
    "    y_test_set,\n",
    ")\n",
    "chosen_model, confusion_matrice = evaluate_random_forest_performance(\n",
    "    grid_search, repartition_dataset\n",
    ")\n",
    "print('columnes matrice confusion {}'.format(list(np.unique(y_test_set))))\n",
    "print(\"matrice de confusion \\n {} \".format(confusion_matrice))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'apercoit ici que l'algorithme arrive bien à classer les <b> Data architecte </b> et les <b> Data engineer </b>\n",
    "<br> Il a cependant plus de mal à distinguer les <b> data scientist </b> des <b> lead data scientist </b> mais moinsque les algorthmes utilisés precedemment à savoir l'arbre de decision et le svm.\n",
    "<br> Un problème se pose pour les autres corps de metiers comme les <b>Data architecte</b> et les <b> Data engineer </b>.\n",
    "<br> En effet, il n'est pas precisé si ceux ci sont des <b>lead</b> dans leur domaines c'est à dire possèdent des qualités manageriales, ce qui peut être le cas vu le nombre d'année d'experience de certains d'eux.\n",
    "<br> Voyons maintenant l'impact  de cette nouvelle variable sur notre <b> matrice de confusion </b> et notre <b> taux de bonne classification </b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metiers_a_predire = prediction_metier_manquants(\n",
    "    metiers_manquant, metier_manquant, chosen_model\n",
    ")\n",
    "\n",
    "# avoir un apercu des predictions à effectuer\n",
    "metiers_a_predire.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Décrire de façon détaillée les différentes étapes pour mener à bien un projet data complexe.\n",
    "\n",
    "Dans le cadre d’une campagne de recrutement vous avez reçu un ensemble de cv qui ne sont pas à jour (le poste actuel n’est pas présent). Deux exemple de CV vous sont donnés en pièce jointe (il n'y a pas de mise en page type). Tous les CV sont au format PDF.\n",
    "\n",
    "On souhaite connaitre le poste actuel de chaque candidat.\n",
    "\n",
    "-  Quelles sont les différentes étapes nécessaires à la réalisation d'un tel projet?\n",
    "-  Pour chaque étape, détaillez les taches à accomplir, les méthodes à suivre, les pistes possibles, les points critiques.\n",
    "-  Justifiez consciencieusement vos choix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differents étapes pour mener a bien un projet data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Les differentes étapes consisteront pour moi \n",
    "<br> <b> CERNER LE PROBLEME ET RECHERCHER UNE VISION D'ENSEMBLE </b>\n",
    "<br> En effet, il est important ici de savoir s'il y'a uniquement une liste de métier particulier\n",
    "que nous souhaitons prédire comme plus haut (data scientist, data architecte etc...) ou si nous souhaitons prédire tout type de métier. Ceci constitue un point critique en ce sens ou le second cas serait plus compliqué en terme d'échantillonnage.\n",
    "<br> Aussi serait t'il important de savoir si nous disposons uniquement de CV dont le metier actuel manque ou si nous avons egalement à disposition des cv complets. Si nous ne disposons pas de CV complets pour notre tâche, il apparait opportun d'en constituer une base à partir de sources comme linkedIn ou tout autre reseau social profesionnel puis de les anonymiser pour être en phase avec la RGPD (protection des donnees personnelles). Cette anonymisation passe par une suppression des données personnelles (nom, prenom, numero de telephone, mail, lien professionnel etc...) dans la phase de mise en oeuvre des features.\n",
    "<br> <b> COLLECTE DES DONNEES </b>\n",
    "<br> Il s'agit ici d'une étape de text-mining. En effet, il faudra traiter les données pour en extraire de l'information. Si nous avons en effet à disposition, <b>un ensemble de compétences</b>, <b>de métiers connus</b> à extraire des cv <b> anonymisés</b>, il nous suffirait simplement ici de les chercher dans le texte et de les extraires pour en faire une base de features.\n",
    "<br> Bien vrai qu'il n'existe pas de standard type pour les cv, ceux ci contiennent en general une section <b> experience</b>, <b>education</b> etc..\n",
    "<br> <b> TRAITEMENT DES DONNEES </b>\n",
    "<br> Une fois les données stockées, il va falloir les analyser, trouver une technique de remplacement des valeurs manquantes comme le nombre d'années d'experiences ( si pas renseignée ou pas calculable). Concernant les compétences, comme on a pu le remarquer au travers de l'exercice precedent, il va falloir traiter les doublons, les écritures differentes etc .....\n",
    "<br> Ensuite transformer les variables categorielles en variables numeriques pour le modèle via des techniques comme le <b>OneHotEncoding</b> , procéder à une reduction de dimension si possible pour accélerer le temps de calcul des modèles puis essayer de créer ou d'ajouter des features (<b> features engineering </b>) et enfin tester des modèles de machine learning. D'abord des modèles simples comme un arbre de decision, un svm puis si les performances ne sont pas satisfaisantes des modèles un peu plus complexe comme les random forest, le boosting etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Definition du probleme\n",
    "\n",
    "2- Acquisition d'une base de donnée de fichiers proches complet (C ) VIA SCRAPPING , BDD interne\n",
    "Attention RGPD\n",
    "\n",
    "3- Consolidation de la BDD, text mining, data cleaning\n",
    "\n",
    "4- LAbelisation des données si necessaire\n",
    "\n",
    "5-Entrainement de modeles ( Train Dev Test) \n",
    "Choix du modele : Explicable ou boite noire \n",
    " CLassifacation du dataset - Features extraction ( NN, Xgboost ...) + Classifier ( SVM, NN, ...)\n",
    "CRoss VAlidation : ATtention a surentrainement\n",
    "\n",
    "6- Prediction du label\n",
    "Prediction mono class, probbabiliste ( softmax )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
