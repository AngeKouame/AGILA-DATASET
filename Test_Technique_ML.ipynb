{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I : Quel(le) data scientist êtes-vous ?\n",
    "## Contexte de l’analyse\n",
    "\n",
    "Elu métier le plus sexy par la Harvard Business Review en octobre 2012, le data scientist représente un profil rare qui exige de nombreuses compétences.\n",
    "\n",
    "A partir d'un dataset Aquila, vous réaliserez :\n",
    "- un clustering non supervisé afin d'identifier 2 groupes de profils techniques distinctes\n",
    "- une prédiction des profils dont le métier n'est pas labellisé\n",
    "\n",
    "\n",
    "## Données\n",
    "data.csv contient 6 variables : \n",
    "    - 'Entreprise' correspond à une liste d'entreprises fictive\n",
    "    - 'Metier' correspond au métier parmi data scientist, lead data scientist, data engineer et data architecte\n",
    "    - 'Technologies' correspond aux compétences maîtrisées par le profil\n",
    "    - 'Diplome' correspond à son niveau scolaire (Bac, Master, PhD,...)\n",
    "    - 'Experience' correspond au nombre d'années d'expériences\n",
    "    - 'Ville' correspond au lieu de travail\n",
    "    \n",
    "\n",
    "\n",
    "## Répondez aux questions \n",
    "\n",
    "Bonne chance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des libraries classique (numpy, pandas, ...)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn as sk\n",
    "import seaborn as sb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics  import *\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0752644800']\n"
     ]
    }
   ],
   "source": [
    "string = 'mayi va appeler cedric au 0752644800'\n",
    "print(re.findall(r'\\d+', string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Importer le tableau de données dans un dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entreprise,Metier,Technologies,Diplome,Experience,Ville\n",
      "\n",
      "Sanofi,Data scientist,Matlab/Python/Pyspark/Scikit-learn/Tensorflow,Master,1,Paris\n",
      "\n",
      "Massachusetts General Hospital(MGH),Data architecte,Python/Java/Scala/MongoDB,Master,3,Marseille\n",
      "\n",
      "Delve,Lead data scientist,SPSS/SQL/Teradata/R/Python/Tensorflow/scikit-learn,Master,3,Nantes\n",
      "\n",
      "Ann & Robert H. Lurie Children’s Hospital of Chicago,Data scientist,C/C++/Java/Python,Master,\"1,5\",Marseille\n",
      "\n",
      "Arm,Data scientist,Matlab/Python/C++/numpy/Tensorflow/scikit-learn,Phd,,Bordeaux\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avoir un apercu des donnees, detecter le separateur\n",
    "# et le caractere utilise pour les virgules\n",
    "ligne_a_lire = 6\n",
    "with open('data.csv','r') as file:\n",
    "    for i in range(ligne_a_lire):\n",
    "        print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le separateur de columne ici est une <b>','</b>. Nous allons l'utiliser avec pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du dataframe \"data.csv\"\n",
    "filename = 'data.csv'\n",
    "df = pd.read_csv(filename, sep=',', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoir un apercu des premieres lignes du fichier\n",
    "nombre_lignes_a_visualiser = 162\n",
    "df.head(nombre_lignes_a_visualiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Combien y a t-il d'observations dans ce dataset? Y a t-il des valeurs manquantes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methode 1 : avec shape\n",
    "# methode 2 : avec len\n",
    "print(' methode 1: nombre d\\'observations du dataset {} \\n'.format(df.shape[0]))\n",
    "print(' methode 2: nombre d\\'observations du dataset {} \\n'.format(len(df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apercu des valeurs manquantes\n",
    "## Calculons la proportion de valeurs manquantes dans chacune des columnes\n",
    "print('proportion des valeurs manquantes dans chacune des columnes en % \\n')\n",
    "print(df.isnull().mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Concernant les valeurs manquantes, je constate que les variables categorielles <b> Entreprise</b>, <b>Metier</b> en contiennent approximativement et respectivement <b>0.17%</b> et <b>0.18%</b> tandis que la variable numerique <b>Experience</b> en contient moins de <b>1%</b>.\n",
    "<br> Ces donnees manquantes sont assez rares (moins de 1%) en general, le dataset est donc exploitable et on peut remplacer ces valeurs manquantes par diverses méthodes (moyenne, medianne, mode (pour les variables categorielles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Réaliser l'imputation des valeurs manquantes pour la variable \"Experience\" avec : \n",
    "- la valeur médiane pour les data scientists\n",
    "- la valeur moyenne pour les data engineers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Realisation de l'imputation de la valeur manquante\n",
    "## Remplacer les valeurs manquantes pour les data scientists par la valeur mediane\n",
    "index_data_scientist = list(df[df.Metier.isin(['Data scientist'])].isnull().index.values)\n",
    "df.loc[index_data_scientist,'Experience'] = df.loc[index_data_scientist,'Experience'].transform(lambda col:col.fillna(col.median()))\n",
    "\n",
    "## Remplacer les valeurs manquantes pour les data engineers par la moyenne\n",
    "index_data_engineer = list(df[df.Metier.isin(['Data engineer'])].isnull().index.values)\n",
    "df.loc[index_data_engineer,'Experience'] = df.loc[index_data_engineer,'Experience'].transform(lambda col:col.fillna(col.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode que j'ai employee ici, consiste d'abord a detecter <b> les indices </b> du dataset correspondant aux differents emplois qu'on recherche ici, à savoir <b> data scientist </b> et <b> data engineers </b>.\n",
    "<br> Ensuite nous allons proceder à la modification des <b> NaN </b> presents à ces ligne par les valeurs demandées.\n",
    "<br> <b> Il existe surement des méthodes plus optimisées mais celle la me parait pas, j'aurai le temps de reflechir à d'autres méthodes et calculer la plus rapide </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Combien d'années d'expériences ont, en moyenne, chacun des profils : le data scientist, le lead data scientist et le data engineer en moyenne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des annees d'experiences de chacun des profils\n",
    "print('Moyenne des experiences des differents metiers \\n {}'.\\\n",
    "      format(df.groupby('Metier')['Experience'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Le data scientist </b> a en moyenne <b>3.0 annees </b> d'experiences. <br>\n",
    "<b> Le lead data scientist </b> a en moyenne <b>3.96 annees </b> d'experiences. <br>\n",
    "<b> Le data engineer </b> a en moyenne <b>2.98 annees </b> d'experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Faire la représentation graphique de votre choix afin de comparer le nombre moyen d'années d'expériences pour chaque métier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Metier')['Experience'].mean().plot(kind='bar')\n",
    "plt.title('Annee d\\'experience de divers acteurs de la data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Transformer la variable continue 'Experience' en une nouvelle variable catégorielle 'Exp_label' à 4 modalités: débutant, confirmé, avancé et expert\n",
    "- Veuillez expliquer votre choix du règle de transformation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va partir du postulat qu'un lead data scientist est un <b> avance </b> <br> En effet ce dernier est amené à gerer une equipe de data scientist debutants( sortant d'ecoles et de confirmés), la valeur mediane du lead data scientist etant de 3.5 et la moyenne de 3.96, on va considere qu'a partir de 4 ans d'experience, un ingénieur (architecte, data scientist etc...) deviens expert dans son domaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorized_variable(value):\n",
    "    '''\n",
    "    INPUT \n",
    "        - value - annees d'experiences\n",
    "    OUTPUT \n",
    "        - categorie choisie\n",
    "    '''\n",
    "    if value<2:\n",
    "        return 'Debutant'\n",
    "    elif value>=2 and value<4:\n",
    "        return 'Confirme'\n",
    "    elif value>=4 and value<=8:\n",
    "        return 'Avance'\n",
    "    else:\n",
    "        return 'Expert'\n",
    "\n",
    "# Creer une variable categorielle à 4 modalites\n",
    "df['Exp_label'] = df['Experience'].apply(categorized_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.Experience.dropna(how='all')\n",
    "plt.hist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Quelles sont les 5 technologies les plus utilisées? Faites un graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listons l'ensemble des technologies utilisées par l'ensemble des candidats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listons l'ensemble des technologies utilisees\n",
    "technologies_utilisees = list(map(lambda x:x.split('/'), df.Technologies))\n",
    "# Mettons à plat (nested) la liste de liste obtenu precedemment\n",
    "liste_des_logiciels_utilises = [val.lower() for sublist in technologies_utilisees for val in sublist if len(val)!=0]\n",
    "\n",
    "# Stockons la liste unique des logiciels utilises\n",
    "liste_unique_des_logiciels_utilises = list(set(liste_des_logiciels_utilises))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_unique_des_logiciels_utilises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre des N technologies desirees\n",
    "liste_n_largest = 5\n",
    "# Stockons dans une liste le nombre\n",
    "sizes = [liste_des_logiciels_utilises.count(elt) for elt in liste_unique_des_logiciels_utilises]\n",
    "# Avoir les index des N technologies les plus utilisees\n",
    "index = np.argsort(sizes)[-liste_n_largest:]\n",
    "# Avoir les noms de ces technologies\n",
    "labels = np.asarray(liste_unique_des_logiciels_utilises, dtype=str)\n",
    "labels = list(labels[index])\n",
    "# Regrouper toutes les autres technologies dans une grosse categorie appelee Autres\n",
    "labels2 = labels\n",
    "labels.append ('Autres') #rajouter les autres qu'on a pas comptabiliser\n",
    "sizes2 =np.asarray(sizes)[index]\n",
    "sizes2 = list(sizes2)\n",
    "sizes2.append(len(liste_des_logiciels_utilises)-sum(sizes2)) \n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes2, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=180)\n",
    "ax1.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "##### je vois l'iddee\n",
    "liste_des_logiciels_utilises2 = np.asarray(liste_des_logiciels_utilises)  \n",
    "liste_des_logiciels_utilises2[~np.isin(liste_des_logiciels_utilises2, labels2)] = 'Autres'   \n",
    "data2 = pd.DataFrame(data=liste_des_logiciels_utilises2, columns=['logiciels'])\n",
    "ax = sb.countplot( y='logiciels', data= data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion <br>Comme on peut le constater sur le camembert, <br> Python vient en première position des logiciels utilisés  par les personnes de la data avec un score de <b>15.3%</b>, suivi de près par le langage R, prisée des statisticiens <b>10.0%</b>, Ensuite viennent des outils de gestion, de gestion de la donnee comme SQL <b>5.9%</b>, Hadoop <b>3.6%</b>. <br> Java lui est moins utilisé comparé à Python et R, cela peut être lié au fait que les outils orientés Machine Learning sont plus rares ou moins vulgarisés (prisés)\n",
    "<br> On constate aussi que des mots comme Anglais sont presents dans les technologies et il existe plusieurs denominations du meme outil comme Spark (pyspark). Aussi, des technologies mentionnees matplotlib et scikit learn font partie de python qui lui meme est mentionne. Nous avons reglé le probleme de la denomination du meme logiciel <br>exemple:  Hadoop et hadoop </br> en mettant en miniscule le nom des technologies.\n",
    "<br> En conclusion, il y'a beaucoup de redondances d'informations. Afin d'effectuer une classification ou une possible regression, nous allons proceder a une reduction de la dimensionnalité du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_des_logiciels_utilises2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Réaliser une méthode de clustering non supervisée de votre choix pour faire apparaître 2 clusters que vous jugerez pertinents. Donnez les caractéristiques de chacun des clusters.\n",
    "-  Justifier la performance de votre algorithme grace à une métrique.\n",
    "-  Interpréter votre resultat.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoriser les technolgies via le one hot encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "software_maitrise = pd.DataFrame(mlb.fit_transform(df.Technologies.str.split('/')),columns=mlb.classes_, index=df.index)\n",
    "software_maitrise = software_maitrise.drop('',axis=1)#element non desire issu du split\n",
    "#software_maitrise = np.asarray(software_maitrise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AI', 'AWS', 'Anglais', 'Big data', 'C', 'C++', 'Cassandra',\n",
       "       'Deep learning', 'Docker', 'Elasticsearch', 'Excel', 'GNU', 'HBASE',\n",
       "       'HDFS', 'Hadoop', 'Hadoop(HDFS)', 'Hive', 'Java', 'Kafka', 'Kibana',\n",
       "       'Linux', 'Machine learning', 'Map-Reduce', 'MariaDB', 'Matlab',\n",
       "       'Matplotlib', 'Microsoft Azure', 'MongoDB', 'MySQL', 'NoSQ', 'NoSQL',\n",
       "       'ORACLE', 'PIG', 'Perl', 'PostgreSQL', 'PySpark', 'Pycharm', 'Pyspark',\n",
       "       'Python', 'R', 'Redshift', 'Ruby', 'SAS', 'SPSS', 'SQL', 'Scala',\n",
       "       'Scikit-learn', 'Scoring', 'Spark', 'Tableau', 'Tensorflow', 'Teradata',\n",
       "       'VBA', 'Vertica', 'Windows', 'Yarn', 'anglais', 'machine learning',\n",
       "       'numpy', 'scikit-learn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software_maitrise.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agira ici de remplacer toute les variables categorielles par des variables numeriques avec le one hot encoding. \n",
    "On peut supprimer la variable metier pour cette etude, par ce que le metier correspont approximativement a une certaine categorie de competences techniques plus ou moins defini, l'entreprise aussi ne semble pas être une variable pertinente, de meme que la ville ou l'emploi est effectué\n",
    "Le nombre d annee d experience peut definir un profil technique on va get dummies la variable cree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software_maitrise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job_experiences = np.asarray(pd.get_dummies(df.Exp_label))\n",
    "diplomes = np.asarray(pd.get_dummies(df.Diplome))\n",
    "metiers = np.asarray(pd.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_of_clustering = np.concatenate((software_maitrise, job_experiences, diplomes), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction de la dimensionnalite concernant les technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "percentage_variance_explained = []\n",
    "for elt in range(1,dataset_of_clustering.shape[1]):\n",
    "    tsvd = TruncatedSVD(n_components=elt)\n",
    "    X =  tsvd.fit(dataset_of_clustering).transform(dataset_of_clustering)\n",
    "    percentage_variance_explained.append(tsvd.explained_variance_ratio_[0:elt].sum())\n",
    " \n",
    "plt.plot(list(range(1, dataset_of_clustering.shape[1])), percentage_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tsvd = TruncatedSVD(n_components=40)\n",
    "software_maitrise_to_take =  tsvd.fit(dataset_of_clustering).transform(dataset_of_clustering)\n",
    "print('pourcentage explique {} \\n'.format(tsvd.explained_variance_ratio_[0:40].sum()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.metrics import davies_bouldin_score \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_of_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters=2, n_init=100)\n",
    "kmeans.fit(dataset_of_clustering)\n",
    "preds = kmeans.fit_predict(dataset_of_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering()\n",
    "clustering.fit(dataset_of_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_validity = silhouette_score(dataset_of_clustering, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmean_label'] = kmeans.labels_\n",
    "df['agglo_label'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Réaliser la prédiction des métiers manquants dans la base de données par l'algorithme de votre choix\n",
    "-  Justifier la performance de votre algorithme grace à une métrique.\n",
    "-  Interpréter votre resultat.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metiers_pas_renseigne = df[pd.isnull(df.Metier)].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base = df.dropna(subset=['Metier'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Décrire de façon détaillée les différentes étapes pour mener à bien un projet data complexe.\n",
    "\n",
    "Dans le cadre d’une campagne de recrutement vous avez reçu un ensemble de cv qui ne sont pas à jour (le poste actuel n’est pas présent). Deux exemple de CV vous sont donnés en pièce jointe (il n'y a pas de mise en page type). Tous les CV sont au format PDF.\n",
    "\n",
    "On souhaite connaitre le poste actuel de chaque candidat.\n",
    "\n",
    "-  Quelles sont les différentes étapes nécessaires à la réalisation d'un tel projet?\n",
    "-  Pour chaque étape, détaillez les taches à accomplir, les méthodes à suivre, les pistes possibles, les points critiques.\n",
    "-  Justifiez consciencieusement vos choix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
